### Метрики. 

Основная задача рекомендательных систем -- ранжирование предложений. То есть не так важна абсолютная величина отзыва, как их отношение.

Для обучения алгоритма SVD я использовала метрики MSE и MAE (именно они предложены в библиотеке [Surprise](https://surprise.readthedocs.io/en/stable/index.html)). Этот алгоритм предсказывает абсолютное значение отзыва, поэтому для него такие метрики уместны. После этого предложения можно ранжировать в согласии с предсказанными значениями и посчитать другие метрики, которые больше подходят для решения именно задачи рекомендательной системы. Также для алгоритма SVD было интересно посчитать метрику R2, которая показывает лучше ли наше решение, чем если бы мы всегда предсказывали среднее значение. Оказывается, что лучше.

Для оценки рекомендации (топ 10) использовались precision_at_k и auc_score.

Для модели LightFM я использовала библиотеку [LightFM](https://making.lyst.com/lightfm/docs/index.html). Метрики для этой модели были реализованы там же. Для SVD метрики были реализованы [тут](https://github.com/NicolasHug/Surprise/blob/master/examples/precision_recall_at_k.py).

### Разбиение данных. 
В качестве простого и быстрого разбиения датасета можно было выбрать случайные 80% данных для train set и оставшиеся 20% для test set (деление 80/20 взято как некоторая стандартная практика). Однако, в данном датасете есть информация о времени, когда пользователь оставил оценку фильма. Поэтому в данном случае возможен следующий вариант: для каждого пользователя взять 80% первых отметок в train и последние 20% в test. При таком разбиении у нас каждый пользователь попадет и в train и в test. Также такое разбиение имеет удобную трактовку: зная предыдущие оценки пользователя, предсказать, какую оценку он даст новому фильму. Стоит заметить, что в таком случае небольшая часть фильмов вообще не окажется в train (фильм вышел недавно, все оценки попали в последние 20% по времени). Такие примеры позволяют увидеть насколько точно алгоритм сможет предсказать оценку пользователя для новых фильмов. 

Для обучения второй модели (LightFM) я использовала дополнительную информацию о фильмах: год выпуска и жанры. Также можно было использовать теги, однако тогда обучение шло слишком медленно (примерно в 80 раз дольше).

### Настройка гиперпараметров.
Для настройки гиперпараметров для обоих алгоритмов я использовала GridSearch. 

### Лучшая модель.
По метрикам лучше получилась модель SVD. Но, наверное, мне просто не хватило времени подобрать хорошие гиперпараметры для LightFM.